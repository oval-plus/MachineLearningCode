不得不说sklearn这个包真厉害啊……感知机写了自己写(chao)的，也用sklearn调包了。自己写的方法和sklearn一比简直是自取其辱。写逻辑回归主要是因为……课上老师让我们用R调包写了，所以在python上复现一下。（统计还是R比较强。python才是世界上最好的语言.jpg） 
<!-- more --> 
# 感知机 #
----------
## 算法 ##
可视化抄的同学的。数据用的是Iris数据库，也没有分测试集和训练集，只实现了原理。这个是二维的，有空的话写写三维的再做个可视化吧，应该还挺好看的。调取数据没有用sklearn内置的iris datasets，如果用了总感觉如蜜传如蜜……所以用了pandas进行了预处理。  

```python
# -*- coding: utf-8 -*-

import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

start=time.process_time()
#pre-process data
path = r'iris.data'
iris=pd.read_csv(path)
iris.columns=['sl','sw','pl','pw','type_raw']
X=iris.iloc[0:,1:3]

Y=iris.type_raw.astype('category')
y1=Y.cat.rename_categories({'Iris-setosa':-1,'Iris-versicolor':1,'Iris-virginica':0})
   #.cat is an attritbute associated with categorical
x=np.c_[X.iloc[0:, 0], X.iloc[0:, 1]] #Arraying
y2=np.asarray(y1)
y =np.where(y2>=0,1,-1)

#function settings
def perceptron(x,y,max_iter=200,lr=0.01):  
    #'max_iter'is the max iteration, 'lr' is the learning rate 
    w = np.zeros(x.shape[1]) #initialize w, b
    b = 0

    for i in range(max_iter):
        y_pre = predict(w,x,b)
        error = y*y_pre # error = 1 or -1
        index = np.argmin(error) #return the index of error's min

        delta = lr*y[index]
        w += delta*x[index]
        b += delta
    return w,b

def predict(w,x,b):
    rs=np.asarray(x,dtype=np.float32).dot(w)+b
    return np.sign(rs).astype(np.float32)

#processing
w,b = perceptron(x,y)
z = predict(w,x,b)
accuracy = sum([z == y] ).mean()
print(accuracy)

end=time.process_time()
print('Processing time:',end-start)
#visualization
m = np.linspace(min(X.iloc[0:,0]),max(X.iloc[0:,0]),100)
n = -w[0]*m/w[1]-b/w[1]
plt.scatter(X.iloc[0:,0], X.iloc[0:,1], c=y, cmap=plt.cm.Paired)
plt.plot(m,n)
plt.show()
```

## sklearn ##
sklearn这包可真是太方便了，薮不出话。分了测试集和训练集。  
官方文档[linear_model.Perceptron在这儿](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)  
```python
# -*- coding: utf-8 -*-

import time
from sklearn import *
from sklearn.linear_model import Perceptron
from sklearn.model_selection import train_test_split
import numpy as np

start=time.process_time()
iris=datasets.load_iris()
X = iris.data[ : , 1:3]  
y = iris.target
y[y==2]=1
y[y==0]=-1
x = np.c_[X[:, 0], X[:, 1]]

p=0.3
def perceptron(p):
    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=p)
    Clif = Perceptron(random_state=0)
    Clif.fit(x_train,y_train)
    KK=Clif.predict(x_test)
    error=np.mean((abs(KK-y_test)))
    return(error)

error_list=[]
for i in range(0,1000):
    error_list.append(perceptron(p))
s=np.mean(error_list)
print(s)

end=time.process_time()
print('Processing time:',end-start)
```
# 逻辑回归 #
没啥好说的，直接sklearn，[官方文档在这儿](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)。主要就是注意LogisticRegression的solver和multinomial。lbfgs存在不收敛的问题，估计可以靠修正数据集来修复。逻辑回归真的挺慢的……还有就是……只要n足够大，error就会足够小，太真实了……
  
```python  
# -*- coding: utf-8 -*-

import time
from sklearn import *
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import numpy as np

start=time.process_time()
X, y = datasets.load_iris(return_X_y=True)

p=0.3
def logistic(p):
    x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=p)
    Clif = LogisticRegression(solver='newton-cg',random_state=0,multi_class='multinomial')
    Clif.fit(x_train,y_train)
    KK=Clif.predict(x_test)
    error=np.mean((abs(KK-y_test)))
    return(error)

error_list=[]
for i in range(0,1000):
    error_list.append(logistic(p))
s=np.mean(error_list)
print(s)

end=time.process_time()
print('Processing time:',end-start)
```  

2019/11/09